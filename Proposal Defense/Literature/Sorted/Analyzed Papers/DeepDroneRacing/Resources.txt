[1]
Note
that moving gates break the main assumption of traditional high-speed navigation approaches [1, 9],
specifically that the trajectory can be pre-planned in a static world.

[2]
... state-of-the-art state estimation pipelines may require expensive sensors [2], ...

[3] 
Simultaneous Localization and Mapping (SLAM) systems [3] can provide accurate pose estimates against a previously-generated, 
globally-consistent map.

[4]
... deriving actions directly from images using end-to-end trainable
machine learning systems [25, 27, 14, 4, 24, 13, 11]. While being independent of any global map
and position estimate, these methods are not directly applicable to our specific problem due to 
... the inherent difficulties of generalizing to 3D motions [25, 4, 24].

[5]
The control system
then uses these outputs to generate a minimum jerk trajectory [20] that is tracked by a low-level
controller [5].

To track ts, i.e., to compute the low-level
control commands, we deploy the control scheme proposed by Faessler et al. [5].

[6]
Ground truth for this [end-to-end low-level commands' regressor] baseline is generated by
the low-level controller in [6] while the drone is tracking the global reference trajectory under the
assumption of accurate state estimation.

[7]
look for the target (the next gate) and localize relative to this while maintaining visual contact
with it [7, 28].

We compare our
approach to the handcrafted window detector of Falanga et al. [7] by replacing our perception system
(Section 3) with the handcrafted detector and leaving the control system (Section 3) unchanged.

[8]
As baseline, we use a pure
feedforward setting by following the global trajectory tg using visual inertial odometry [8].

[9]
Note
that moving gates break the main assumption of traditional high-speed navigation approaches [1, 9],
specifically that the trajectory can be pre-planned in a static world.

[10]
We use RotorS [10] and Gazebo for all simulation experiments.

[11]
... deriving actions directly from images using end-to-end trainable
machine learning systems [25, 27, 14, 4, 24, 13, 11]. While being independent of any global map
and position estimate, these methods are not directly applicable to our specific problem due to their
high computational complexity [27, 11], ...

[12]
... our approach can benefit from years of study in the field of control theory [12].

[13]
... deriving actions directly from images using end-to-end trainable
machine learning systems [25, 27, 14, 4, 24, 13, 11]. While being independent of any global map
and position estimate, these methods are not directly applicable to our specific problem due to their
... low maximum speed [13] ... 

[14]
... deriving actions directly from images using end-to-end trainable
machine learning systems [25, 27, 14, 4, 24, 13, 11].

the optimal output representation for
learning-based algorithms that couple perception and control is an open question. Known output
representations range from ... to direct control [14]—which can lead to highly agile control,
but suffers from high sample complexity.

[15]
the optimal output representation for
learning-based algorithms that couple perception and control is an open question. Known output
representations range from predicting discrete navigation commands [15, 17]— which enables high
robustness but leads to low agility ...

[16]
To fully understand the potential and limitations of our approach we compared to a
diverse set of baselines, such as a classic approach based on planning and tracking [16]...

For our baseline, we use a state-of-the-art visual-inertial odometry approach [16] to provide global
state estimates in order to track the global reference trajectory.

[17]
the optimal output representation for
learning-based algorithms that couple perception and control is an open question. Known output
representations range from predicting discrete navigation commands [15, 17]— which enables high
robustness but leads to low agility ...

To allow for onboard computing, we employ the efficient ResNet-based architecture of Loquercio et
al. [17]

[18]
... state-of-the-art state estimation pipelines may ... be subject to drift due to the use of compressed maps [18].

[19]
This makes it extremely difficult to fully exploit the properties
of popular minimum-snap or minimum-jerk trajectories [19, 20] for small, agile quadrotors using
only onboard sensing and computing.

Expert policy. We first compute a global trajectory tg that passes through all gates of the track, using
the minimum-snap trajectory implementation from Mellinger and Kumar [19].

[20]
This makes it extremely difficult to fully exploit the properties
of popular minimum-snap or minimum-jerk trajectories [19, 20] for small, agile quadrotors using
only onboard sensing and computing.

The control system
then uses these outputs to generate a minimum jerk trajectory [20] that is tracked by a low-level
controller [5].

[21]
We use this track to compare the performance to a deep
learning baseline that directly regresses body rates from raw images [21].

In contrast
to previous work [21], we believe that end-to-end learning of low-level controls is suboptimal for the
task of drone navigation when operating in the real world.

[22]
... state-of-the-art state estimation pipelines may ... have high computational costs [22], ...

[23]
Indeed, the [end-to-end] network has to learn the basic
notion of stability from scratch in order to control an unstable platform such as a quadrotor [23].

[24]
... deriving actions directly from images using end-to-end trainable
machine learning systems [25, 27, 14, 4, 24, 13, 11]. While being independent of any global map
and position estimate, these methods are not directly applicable to our specific problem due to 
... the inherent difficulties of generalizing to 3D motions [25, 4, 24].

[25]
... deriving actions directly from images using end-to-end trainable
machine learning systems [25, 27, 14, 4, 24, 13, 11]. While being independent of any global map
and position estimate, these methods are not directly applicable to our specific problem due to 
... the inherent difficulties of generalizing to 3D motions [25, 4, 24].

[26]
In simulation, we employed a variant of DAgger [26],
which uses the expert policy to recover whenever the learned policy deviates far from the reference
trajectory.

[27]
... deriving actions directly from images using end-to-end trainable
machine learning systems [25, 27, 14, 4, 24, 13, 11]. While being independent of any global map
and position estimate, these methods are not directly applicable to our specific problem due to their
high computational complexity [27, 11], ...

[28]
look for the target (the next gate) and localize relative to this while maintaining visual contact
with it [7, 28].

[29]
Figure 8: Visualization of network attention using the Grad-CAM technique [29].

[30]
However, traditional, handcrafted gate detectors quickly become unreliable in the
presence of occlusions, partial visibility, and motion blur. The classical solution to this problem is
visual servoing, where a robot is given a set of target locations in the form of reference images [30].